---
title: "Quantified Self's Not Dead: Crossfit for your QS Data"
author: "S Duffy"
date: "May 13, 2016"
output: html_document
---

##Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. This project will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


##Data Exploration

```{r loadnlook, }
training <- read.csv("C:/Users/stephen.p.duffy/Desktop/pml-training.csv")
testing <- read.csv("C:/Users/stephen.p.duffy/Desktop/pml-testing.csv")

d <- dim(training)
h <- head(training[,1:10])
s <- str(training$classe)
```

I won't run any code because it takes a long time to train models.  The code below can be executed by the reader if desired.  The training data has 19,622 rows and 160 columns. The Classe variable, which will be what I attempt to predict, is a factor with five levels A through E.  The data includes a user name, some datestamps, the classe variable and data from the fitness device as seen here.

##Cross Validation

To ensure a better model, this study will use the training dataset provided and break it into
a sub-training set (trainer) equal to 75% of the larger set and a test set (tester) equalling the remaining 25%.  I will use three iterations of this method to create the model.

```{r xvalidation}
library(caret)
set.seed(62433)
inTrain <- createDataPartition(training$classe, p = 3/4, list=FALSE)
trainer <- training[ inTrain,]
tester <- training[-inTrain,]
inTrain2 <- createDataPartition(training$classe, p = 3/4, list=FALSE)
trainer2 <- training[ inTrain2,]
tester2 <- training[-inTrain2,]
inTrain3 <- createDataPartition(training$classe, p = 3/4, list=FALSE)
trainer3 <- training[ inTrain3,]
tester3 <- training[-inTrain3,]
```

##Out of Sample Error

I would assume the out of sample error rate to be low because fitness routines are just that, routine.  Most people aren't inventing anything new so the exercises performed by the users would be relatively standard. The people who perform incorrectly will increase the error rate some. I will use the accuracy rating to rate model effectiveness. Accuracy is the opposite of the out of sample error rate and I think accuracy will be above 90%.

##Model Build

I chose to start with a random forest model because it is one of the most effective models known. Perusing the dataset, I excluded any variables that had missing data, this left me with variables I grouped into sets based on what was doing the measuring.  These sets follow and are available for the belt, arm, and dumbbell recording device of our users: 

* Roll/pitch/yaw
* Accelerometer
* Gyroscope
* Magnets

I fit a model using the dependent variable Classe against each of the above sets, using the input for each recording device.  For instance, I pitted Classe against roll/pitch/yaw of the belt, arm, and dumbell devices and then against the x, y, and z accelerometer values for the belt, arm, and dumbell devices, and so on. I then ran one model of roll/pitch/yaw with the addition of the user's name, thinking each person may perform specific activities or do them in a specific way.  I also ran one model using boosting (gbm) to compare how that did against random forest. It wasn't as good so I stayed using random forest. 

For the three models that had the highest accuracy (all above 90% where the remainder were below 90%), I created additional models using the second and third training and test sets and averaged the accuracies to determine the model I should use (this may not be the best way but these models take a long time to run).

```{r winningmodel}
set.seed(2317)

##This is the winning model
mod1 <- train(classe ~ roll_belt + pitch_belt +yaw_belt + roll_arm + pitch_arm + yaw_arm + roll_dumbbell + pitch_dumbbell + yaw_dumbbell,method="rf",data=trainer)
pred1 <- predict(mod1,tester)
cm1 <- confusionMatrix(pred1,tester$classe)  ##.9653 Acc
cm1$overall['Accuracy']
cm1$table
```

It turns out the best fit is roll/pitch/yaw without the user included, with an average accuracy of 99.06%.  I will use this model to predict against the provided test set.  The code below shows all the models attempted and the average accuracy of the three top models.

```{r models, eval=FALSE}
set.seed(2317)

##These are all the models attempted
mod1 <- train(classe ~ roll_belt + pitch_belt +yaw_belt + roll_arm + pitch_arm + yaw_arm + roll_dumbbell + pitch_dumbbell + yaw_dumbbell,method="rf",data=trainer)
pred1 <- predict(mod1,tester)
cm1 <- confusionMatrix(pred1,tester$classe)  ##.9653 Acc
cm1$overall['Accuracy']

mod12 <- train(classe ~ roll_belt + pitch_belt +yaw_belt + roll_arm + pitch_arm + yaw_arm + roll_dumbbell + pitch_dumbbell + yaw_dumbbell,method="rf",data=trainer2)
pred12 <- predict(mod12,tester)
cm12 <- confusionMatrix(pred12,tester2$classe)  ##.9921 Acc
cm12$overall['Accuracy']

mod13 <- train(classe ~ roll_belt + pitch_belt +yaw_belt + roll_arm + pitch_arm + yaw_arm + roll_dumbbell + pitch_dumbbell + yaw_dumbbell,method="rf",data=trainer3)
pred13 <- predict(mod13,tester)
cm13 <- confusionMatrix(pred13,tester3$classe)  ##.9906 Acc
cm13$overall['Accuracy']

#These models were not as effective based on accuracy against the tester dataset
mod2 <- train(classe ~ user_name + roll_belt + pitch_belt +yaw_belt + roll_arm + pitch_arm + yaw_arm + roll_dumbbell + pitch_dumbbell + yaw_dumbbell,method="rf",data=trainer)
pred2 <- predict(mod2,tester)
cm2 <- confusionMatrix(pred2,tester$classe)  ##.9639 Acc
cm2$overall['Accuracy']

mod22 <- train(classe ~ user_name + roll_belt + pitch_belt +yaw_belt + roll_arm + pitch_arm + yaw_arm + roll_dumbbell + pitch_dumbbell + yaw_dumbbell,method="rf",data=trainer2)
pred22 <- predict(mod22,tester2)
cm22 <- confusionMatrix(pred22,tester2$classe)  ##.9639 Acc
cm22$overall['Accuracy']

mod23 <- train(classe ~ user_name + roll_belt + pitch_belt +yaw_belt + roll_arm + pitch_arm + yaw_arm + roll_dumbbell + pitch_dumbbell + yaw_dumbbell,method="rf",data=trainer3)
pred23 <- predict(mod23,tester3)
cm23 <- confusionMatrix(pred23,tester3$classe)  ##.9615 Acc
cm23$overall['Accuracy']

mod3 <- train(classe ~ user_name + roll_belt + pitch_belt +yaw_belt + roll_arm + pitch_arm + yaw_arm + roll_dumbbell + pitch_dumbbell + yaw_dumbbell,method="gbm",data=trainer,verbose=FALSE)
pred3 <- predict(mod3,tester)
cm3 <- confusionMatrix(pred3,tester$classe)  ##.8917 Acc
cm3$overall['Accuracy']

mod4 <- train(classe ~ user_name + accel_belt_x + accel_belt_y + accel_belt_z + accel_arm_x + accel_arm_y + accel_arm_z + accel_dumbbell_x + accel_dumbbell_y + accel_dumbbell_z ,method="rf",data=trainer)
pred4 <- predict(mod4,tester)
cm4 <- confusionMatrix(pred4,tester$classe)  ##.898 Acc
cm4$overall['Accuracy']

mod5 <- train(classe ~ user_name + gyros_belt_x + gyros_belt_y + gyros_belt_z + gyros_arm_x + gyros_arm_y + gyros_arm_z + gyros_dumbbell_x + gyros_dumbbell_y + gyros_dumbbell_z ,method="rf",data=trainer)
pred5 <- predict(mod5,tester)
cm5 <- confusionMatrix(pred5,tester$classe)  ##.8548 Acc
cm5$overall['Accuracy']

mod6 <- train(classe ~ user_name + magnet_belt_x + magnet_belt_y + magnet_belt_z + magnet_arm_x + magnet_arm_y + magnet_arm_z + magnet_dumbbell_x + magnet_dumbbell_y + magnet_dumbbell_z ,method="rf",data=trainer)
pred6 <- predict(mod6,tester)
cm6 <- confusionMatrix(pred6,tester$classe)  ##.92598 Acc
cm6$overall['Accuracy']

mod62 <- train(classe ~ user_name + magnet_belt_x + magnet_belt_y + magnet_belt_z + magnet_arm_x + magnet_arm_y + magnet_arm_z + magnet_dumbbell_x + magnet_dumbbell_y + magnet_dumbbell_z ,method="rf",data=trainer2)
pred62 <- predict(mod62,tester)
cm62 <- confusionMatrix(pred62,tester2$classe)  ##.9804 Acc
cm62$overall['Accuracy']

mod63 <- train(classe ~ user_name + magnet_belt_x + magnet_belt_y + magnet_belt_z + magnet_arm_x + magnet_arm_y + magnet_arm_z + magnet_dumbbell_x + magnet_dumbbell_y + magnet_dumbbell_z ,method="rf",data=trainer3)
pred63 <- predict(mod63,tester)
cm63 <- confusionMatrix(pred63,tester3$classe)  ##.9833 Acc
cm63$overall['Accuracy']

mean(cm13$overall['Accuracy'],cm12$overall['Accuracy'],cm1$overall['Accuracy'])
##[1] 0.9906199  -- Winner!
mean(cm23$overall['Accuracy'],cm22$overall['Accuracy'],cm2$overall['Accuracy'])
##[1] 0.96146
mean(cm63$overall['Accuracy'],cm62$overall['Accuracy'],cm6$overall['Accuracy'])
##[1] 0.983279
```

Finally, I will run the model against the testing data, to predict which workout is being performed.

```{r predict}
predtest <- predict(mod1,testing)
predtest
```







